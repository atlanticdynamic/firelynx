# MCP Data Processing Service using Starlark
# This example demonstrates JSON data processing and analysis using Starlark (Python-like syntax)

[[listeners]]
id = "data-listener"
address = ":8081"
type = "http"

[[endpoints]]
id = "data-endpoint"
listener_id = "data-listener"

[[endpoints.routes]]
app_id = "data-processor"
[endpoints.routes.http]
path_prefix = "/mcp"
method = "*"

# MCP Data Processing Application
[[apps]]
id = "data-processor"
type = "mcp"

[apps.mcp]
server_name = "Firelynx Data Processor"
server_version = "1.0.0"

[apps.mcp.transport]
sse_enabled = false

# JSON Data Analyzer Tool
[[apps.mcp.tools]]
name = "analyze_json"
description = "Analyze JSON data structures and extract insights"

[apps.mcp.tools.script]
[apps.mcp.tools.script.static_data]
max_depth = 10
max_items = 1000
analysis_types = ["structure", "statistics", "validation"]

[apps.mcp.tools.script.starlark]
code = '''
def analyze_json():
    """Analyze JSON data and return structural insights"""
    
    # Get data from MCP arguments
    args = ctx.get("args", {})
    data = args.get("data", {})
    analysis_type = args.get("type", "structure")
    
    if not data:
        return {"error": "No data provided. Please include a 'data' parameter with JSON content."}
    
    # Get configuration
    max_depth = ctx.get("max_depth", 10)
    max_items = ctx.get("max_items", 1000)
    
    if analysis_type == "structure":
        return analyze_structure(data, max_depth)
    elif analysis_type == "statistics":
        return analyze_statistics(data)
    elif analysis_type == "validation":
        return validate_data(data, max_items)
    else:
        return {"error": "Unknown analysis type: {}. Supported: structure, statistics, validation".format(analysis_type)}

def analyze_structure(data, max_depth, depth=0):
    """Recursively analyze data structure"""
    if depth > max_depth:
        return {"error": "Maximum depth exceeded"}
    
    if type(data) == "dict":
        keys = list(data.keys())
        nested_objects = []
        for key in keys[:10]:  # Limit to first 10 keys for safety
            value_type = type(data[key])
            if value_type == "dict":
                nested_objects.append(key)
        
        return {
            "text": "Object with {} keys".format(len(keys)),
            "type": "object",
            "key_count": len(keys),
            "keys": keys[:10],  # Show first 10 keys
            "nested_objects": nested_objects,
            "depth": depth
        }
    
    elif type(data) == "list":
        if len(data) == 0:
            return {"text": "Empty array", "type": "array", "length": 0}
        
        # Analyze first few items
        item_types = []
        for item in data[:5]:
            item_types.append(type(item))
        
        return {
            "text": "Array with {} items".format(len(data)),
            "type": "array", 
            "length": len(data),
            "item_types": item_types,
            "sample_items": data[:3]  # Show first 3 items
        }
    
    else:
        return {
            "text": "Primitive value: {}".format(str(data)),
            "type": type(data),
            "value": data
        }

def analyze_statistics(data):
    """Generate statistics about the data"""
    stats = {
        "total_keys": 0,
        "total_values": 0,
        "data_types": {},
        "max_string_length": 0,
        "total_arrays": 0,
        "total_objects": 0
    }
    
    def count_recursive(obj):
        if type(obj) == "dict":
            stats["total_objects"] += 1
            for key, value in obj.items():
                stats["total_keys"] += 1
                count_recursive(value)
        elif type(obj) == "list":
            stats["total_arrays"] += 1
            for item in obj:
                count_recursive(item)
        else:
            stats["total_values"] += 1
            value_type = type(obj)
            if value_type in stats["data_types"]:
                stats["data_types"][value_type] += 1
            else:
                stats["data_types"][value_type] = 1
            
            if type(obj) == "string" and len(obj) > stats["max_string_length"]:
                stats["max_string_length"] = len(obj)
    
    count_recursive(data)
    
    return {
        "text": "Statistics: {} keys, {} values, {} objects, {} arrays".format(
            stats["total_keys"], stats["total_values"], 
            stats["total_objects"], stats["total_arrays"]
        ),
        "statistics": stats
    }

def validate_data(data, max_items):
    """Validate data structure and content"""
    issues = []
    
    def validate_recursive(obj, path="root"):
        if type(obj) == "dict":
            if len(obj) > max_items:
                issues.append("Object at {} has too many keys: {}".format(path, len(obj)))
            
            for key, value in obj.items():
                if type(key) != "string":
                    issues.append("Non-string key at {}: {}".format(path, key))
                validate_recursive(value, path + "." + str(key))
        
        elif type(obj) == "list":
            if len(obj) > max_items:
                issues.append("Array at {} has too many items: {}".format(path, len(obj)))
            
            for i, item in enumerate(obj):
                validate_recursive(item, path + "[{}]".format(i))
        
        elif type(obj) == "string":
            if len(obj) > 10000:  # Very long strings
                issues.append("Very long string at {}: {} characters".format(path, len(obj)))
    
    validate_recursive(data)
    
    if len(issues) == 0:
        return {"text": "Data validation passed - no issues found", "valid": True}
    else:
        return {
            "text": "Data validation found {} issues".format(len(issues)),
            "valid": False,
            "issues": issues[:10]  # Limit to first 10 issues
        }

# Execute the main function
analyze_json()
'''

# Data Transformation Tool
[[apps.mcp.tools]]
name = "transform_data"
description = "Transform and manipulate data structures"

[apps.mcp.tools.script]
[apps.mcp.tools.script.static_data]
max_operations = 100

[apps.mcp.tools.script.starlark]
code = '''
def transform_data():
    """Transform data based on specified operations"""
    
    args = ctx.get("args", {})
    data = args.get("data", {})
    operations = args.get("operations", [])
    
    if not data:
        return {"error": "No data provided"}
    
    if not operations:
        return {"error": "No operations specified. Example: ['sort', 'unique', 'reverse']"}
    
    max_ops = ctx.get("max_operations", 100)
    if len(operations) > max_ops:
        return {"error": "Too many operations specified (max: {})".format(max_ops)}
    
    result = data
    applied_operations = []
    
    for op in operations:
        if op == "sort" and type(result) == "list":
            result = sorted(result)
            applied_operations.append("sorted array")
        
        elif op == "reverse" and type(result) == "list":
            result = list(reversed(result))
            applied_operations.append("reversed array")
        
        elif op == "unique" and type(result) == "list":
            # Remove duplicates while preserving order
            unique_items = []
            seen = {}
            for item in result:
                item_str = str(item)
                if item_str not in seen:
                    unique_items.append(item)
                    seen[item_str] = True
            result = unique_items
            applied_operations.append("removed duplicates")
        
        elif op == "keys" and type(result) == "dict":
            result = list(result.keys())
            applied_operations.append("extracted keys")
        
        elif op == "values" and type(result) == "dict":
            result = list(result.values())
            applied_operations.append("extracted values")
        
        elif op == "flatten" and type(result) == "list":
            flattened = []
            for item in result:
                if type(item) == "list":
                    flattened.extend(item)
                else:
                    flattened.append(item)
            result = flattened
            applied_operations.append("flattened arrays")
        
        else:
            return {"error": "Unknown or incompatible operation '{}' for data type '{}'".format(op, type(result))}
    
    return {
        "text": "Applied {} operations: {}".format(len(applied_operations), ", ".join(applied_operations)),
        "result": result,
        "operations_applied": applied_operations,
        "result_type": type(result)
    }

transform_data()
'''